{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import django\n",
    "\n",
    "os.environ['DJANGO_SETTINGS_MODULE']='TweetToEmoji.settings'\n",
    "\n",
    "django.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.models import Emoji\n",
    "from nltk.corpus import wordnet \n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TweetsFromResponse(response):\n",
    "    return [t.get('text') for t in response.get('statuses')]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from django.db.models import Q\n",
    "def SearchEmoji(keyword):\n",
    "    emoji = Emoji.objects.filter(Q(Name__contains = keyword) | Q(Description__contains = keyword))\n",
    "    return ''.join([e.Icon+str(e.Id)+e.Name for e in emoji])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindSynonyms(word):\n",
    "    syns = wordnet.synsets(word) \n",
    "    synonym_list =[s.lemma_names() for s in syns ] # take synonyms name\n",
    "    synonym_list = [item.replace('_',' ') for sublist in synonym_list for item in sublist] # flatten list\n",
    "    synonym_list= set(synonym_list) # remove duplicates\n",
    "    return list(synonym_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TwitterService.Client import TwitterClient\n",
    "\n",
    "client = TwitterClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = client.Get('search/tweets.json', lang = 'en', q='climate', result_type = 'popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "content = client.GetFromHistory(\"04-11-2018 17.06.12.391\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['People are going to die if we donâ€™t start addressing climate change ASAP.\\n\\nItâ€™s not enough to think itâ€™s â€œimportantâ€¦ https://t.co/fzfg6ppv1y',\n",
       " 'Since this was released on a day no one would notice it... as in a Friday during a holiday weekend..letâ€™s make sureâ€¦ https://t.co/P71S5PDLfZ',\n",
       " 'A massive federal climate change report that was due next month is out today. It warns of devastating health and ecâ€¦ https://t.co/nT1pqtrsW4',\n",
       " 'Try as the Trump Administration might to bury this study on the Friday after Thanksgiving, the effects of climate câ€¦ https://t.co/GhoDU4ewrg',\n",
       " 'The Trump administration tried to bury a new report about the devastating consequences of climate change. \\n\\nWhy? Beâ€¦ https://t.co/tJCcNtVlVi',\n",
       " 'For your records, @realDonaldTrump. \\n\\nYOUR governmentâ€™s report on climate report. \\n\\nYou should really read this. Itâ€¦ https://t.co/N9WljPRlGV',\n",
       " 'The climate crisis threatens both our communities &amp; our economy. While @realDonaldTrump ignores his own adminâ€™s repâ€¦ https://t.co/QtdZGFI0qa',\n",
       " \"Dereliction of duty. Grave words I grant you, but we've had decades of inaction in the face of overwhelming scientiâ€¦ https://t.co/PYSIo06Pxb\",\n",
       " \"Trump administration's first report on climate change impacts on U.S. sees damages â€˜intensifying across the countryâ€™ https://t.co/lutBgwm1WA\",\n",
       " 'Dear @realDonaldTrump,\\n\\nPlease read this new report on climate change, mandated by Congress and made public by yourâ€¦ https://t.co/YejDz87Lof',\n",
       " \"Trump administration climate report says damages are â€˜intensifying across the country' https://t.co/4wK0Tv5I2L\",\n",
       " 'New report show climate chaos is already wreaking havoc on our planet, but @realDonaldTrumpâ€™s Administration wantedâ€¦ https://t.co/cgCFxXR5EL',\n",
       " \"Breaking News: Climate change could slash the size of America's economy by 10 percent by 2100 unless major action iâ€¦ https://t.co/GvwwwgRPgN\",\n",
       " 'â€œAll I want is for people to have healthcare.\\nI want kids to go to college.\\nI want our climate to be saved from catâ€¦ https://t.co/240soWieqJ',\n",
       " 'JUST IN: The U.S. National Climate Assessment is now public.\\n\\nThis is the report that the Trump Administration doesâ€¦ https://t.co/7TFBJ94q13']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = TweetsFromResponse(content)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Burns',\n",
       " 'George Burns',\n",
       " 'Nathan Birnbaum',\n",
       " 'Robert Burns',\n",
       " 'bite',\n",
       " 'burn',\n",
       " 'burn down',\n",
       " 'burn mark',\n",
       " 'burn off',\n",
       " 'burn up',\n",
       " 'burning',\n",
       " 'cauterise',\n",
       " 'cauterize',\n",
       " 'combust',\n",
       " 'cut',\n",
       " 'fire',\n",
       " 'glow',\n",
       " 'incinerate',\n",
       " 'sting',\n",
       " 'sunburn',\n",
       " 'suntan',\n",
       " 'tan'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FindSynonyms(\"burns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TagOf(word):\n",
    "    return pos_tag([s])[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs is NN\n",
      "track(NN) -> ðŸŒ¸1729 Cherry BlossomðŸšˆ1926 Light RailðŸš1929 MonorailðŸ›¤1954 Railway TrackðŸšŸ1976 Suspension RailwayðŸ–²2171 TrackballðŸ”–2207 BookmarkðŸ”€2390 Shuffle Tracks ButtonðŸ”2391 Repeat ButtonðŸ”‚2392 Repeat Single Buttonâ­2395 Next Track Buttonâ®2399 Last Track Button\n",
      "wienerwurst(NN) -> \n",
      "weenie(NN) -> \n",
      "blackguard(NN) -> \n",
      "give chase(NN) -> \n",
      "frank(NN) -> \n",
      "tag(NN) -> ðŸŒ·1737 TulipðŸ›‘1960 Stop Signâš¡2052 High Voltage#2448 Keycap Number SignðŸ”£2465 Input Symbols\n",
      "click(NN) -> \n",
      "dog(NN) -> ðŸ¥º90 Pleading FaceðŸ™‡844 Person BowingðŸ¦´1554 BoneðŸ¶1629 Dog FaceðŸ•1630 DogðŸ©1631 PoodleðŸ¾1678 Paw PrintsðŸŒ­1793 Hot Dog\n",
      "heel(NN) -> ðŸ¤¸1220 Person CartwheelingðŸ¤¸1226 Man CartwheelingðŸ¤¸1232 Woman CartwheelingðŸ¥¿1612 Flat ShoeðŸ‘ 1613 High-Heeled ShoeðŸ‘¡1614 Womanâ€™s SandalðŸ‘¢1615 Womanâ€™s BootðŸ­1835 LollipopðŸŽ¡1916 Ferris WheelðŸ›´1949 Kick ScooterðŸ§³1983 LuggageðŸŽ°2119 Slot Machineâ™¿2320 Wheelchair Symbolâ˜¸2369 Wheel of Dharma\n",
      "andiron(NN) -> \n",
      "hound(NN) -> \n",
      "pawl(NN) -> \n",
      "frankfurter(NN) -> \n",
      "tail(NN) -> ðŸ‘§142 GirlðŸ‘©160 WomanðŸ’1627 MonkeyðŸ©1631 PoodleðŸˆ1636 CatðŸ¿1670 ChipmunkðŸ¦•1701 SauropodðŸ¦–1702 T-RexðŸ¸1845 Cocktail GlassðŸ¹1846 Tropical DrinkðŸ¦1886 BankðŸ†2082 TrophyðŸ“‡2248 Card IndexðŸ”­2293 Telescope\n",
      "bounder(NN) -> \n",
      "cad(NN) -> ðŸ‘¾104 Alien MonsterðŸ¥‘1767 AvocadoðŸ«1833 Chocolate Bar\n",
      "wiener(NN) -> \n",
      "Canis familiaris(NN) -> \n",
      "frump(NN) -> \n",
      "firedog(NN) -> \n",
      "detent(NN) -> \n",
      "chase(NN) -> ðŸ›1607 Shopping BagsðŸ“¼2185 Videocassette\n",
      "go after(NN) -> \n",
      "chase after(NN) -> \n",
      "hot dog(NN) -> ðŸŒ­1793 Hot Dog\n",
      "dog-iron(NN) -> \n",
      "domestic dog(NN) -> \n",
      "hotdog(NN) -> \n",
      "trail(NN) -> \n"
     ]
    }
   ],
   "source": [
    "word=\"dogs\"\n",
    "syn = FindSynonyms(word)\n",
    "print(word +' is ' + TagOf(word))\n",
    "for s in syn:\n",
    "    print('{0}({1}) -> {2}'.format(s, TagOf(s), SearchEmoji(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\leven\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateCombinations(words):\n",
    "    list = []\n",
    "    for slen in range(1, len(words)):\n",
    "        for sshift in range(0, len(words) - slen):\n",
    "            list.append(words[sshift:slen + sshift])\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(sentence):\n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    return nltk.pos_tag(tokenized_text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertTweet(tweet):\n",
    "    sentences = sent_tokenize(tweet)\n",
    "    words=Tokenize(sentences[0])\n",
    "    combinations = GenerateCombinations(words)\n",
    "    combinations = sorted(combinations, key=len, reverse=True)\n",
    "    print(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Do', 'VBP'), ('you', 'PRP'), ('like', 'IN'), ('blowing', 'VBG'), ('bubbles', 'NNS')], [('Do', 'VBP'), ('you', 'PRP'), ('like', 'IN'), ('blowing', 'VBG')], [('you', 'PRP'), ('like', 'IN'), ('blowing', 'VBG'), ('bubbles', 'NNS')], [('Do', 'VBP'), ('you', 'PRP'), ('like', 'IN')], [('you', 'PRP'), ('like', 'IN'), ('blowing', 'VBG')], [('like', 'IN'), ('blowing', 'VBG'), ('bubbles', 'NNS')], [('Do', 'VBP'), ('you', 'PRP')], [('you', 'PRP'), ('like', 'IN')], [('like', 'IN'), ('blowing', 'VBG')], [('blowing', 'VBG'), ('bubbles', 'NNS')], [('Do', 'VBP')], [('you', 'PRP')], [('like', 'IN')], [('blowing', 'VBG')], [('bubbles', 'NNS')]]\n"
     ]
    }
   ],
   "source": [
    "ConvertTweet(tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16.0,
    "lenType": 16.0,
    "lenVar": 40.0
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "405.844px",
    "left": "614.141px",
    "right": "20px",
    "top": "131px",
    "width": "480px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
